<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">this AI is a little bit *TOO* good...</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<p>Matt Schumer, the CEO and founder of Hyperr AI, has found himself in hot water recently. If you've been confused about what's going on with <strong>Reflection 70b</strong> and all the surrounding drama, let me break it down for you.</p>

<p>Not too long ago, Matt announced <strong>Reflection 70b</strong>, claiming it's the world's top <strong>open-source model</strong>. He posted benchmarks that were shockingly good—so good they were surpassing models like <strong>GPT-40</strong>, <strong>Gemini 1.5 Pro</strong>, <strong>Llama 3.1</strong>, <strong>Claude 3.5</strong>, and <strong>Sonic</strong>.</p>

<h2>Extraordinary Claims about Reflection 70b</h2>

<p>This was important because he was saying that an open-source model was surpassing all the big frontier models we've heard about. Reflection 70b is a fine-tuned model, suggesting there's some special <strong>secret sauce</strong> we might use to create our own benchmark-breaking models.</p>

<p>The special sauce is called <strong>reflection tuning</strong>, where the model thinks about and corrects itself to present the correct answer. Matt said the model is based on Meta's latest <strong>Llama 3.1</strong> and was developed with assistance from <strong>Glaive AI</strong>. He emphasized that Glaive AI was the reason this worked so well—the control they give to generate synthetic data is insane. He even suggested others should use them.</p>

<h2>Initial Community Reception</h2>

<p>After his announcement, the community greeted him with open arms. Everyone was celebrating this big breakthrough. <strong>Clem</strong> from Hugging Face, a big name in the AI community, highlighted that <strong>Reflection Llama 3.1 70b</strong> was the number one trending thing on Hugging Face.</p>

<p>He emphasized that you don't need big tech to fine-tune, optimize, and run your own models for your specific constraints—you'll benefit massively from it. Meaning, all of us can build these models, fine-tune them, and create custom models for our own special use cases. If we can do that with models that are competing with the frontier models by big tech companies—that's the dream, isn't it?</p>

<p>Matthew Berman interviewed Matt Schumer and <strong>Sahil</strong>, who is on the Glaive AI side. Many people in the space were augmenting this message, celebrating Matt for providing this incredible new product. A lot of people got caught up in this announcement. Just look at the number of views that original announcement got: <strong>3.3 million views</strong>. This was a big deal.</p>

<h2>Questions and Skepticism Arise</h2>

<p>But then, questions started to emerge. Users testing the model found it didn't perform well at all. Despite the big claims and benchmark results, the model should have been able to perform impressively when tested—but it didn't.</p>

<p>Many people couldn't replicate the results. They started questioning the model's authenticity and performance. Matt Schumer offered a private API key to allow people to test the model, claiming that the open weights model wasn't the correct one—something happened during the upload.</p>

<p>He said they might have been <strong>rate-limited by Hugging Face</strong> mid-upload, possibly uploading parts of two different models somehow. He promised a simple fix, but for some reason, it wasn't working. So they began training a <strong>brand-new model</strong>.</p>

<h2>Investigation into the Model's Authenticity</h2>

<h3>Discrepancies in Model Performance</h3>

<p>The Reddit community <strong>Local Llama</strong> started diving deeper. They found that <strong>Reflection Llama 3.1 70b</strong> appeared to be <strong>Llama 3</strong> with <strong>LoRA</strong> (Low-Rank Adaptation) tuning applied, not Llama 3.1. People noticed that Matt didn't seem familiar with LoRA, which was odd for someone working with these models.</p>

<p>He responded by saying he misunderstood the term in context but had posted about LoRA before. Indeed, he had previously discussed LoRA and <strong>QLoRA</strong> (Quantized Low-Rank Adaptation) in his posts.</p>

<h3>Evidence Suggesting Misrepresentation</h3>

<p>Further investigation suggested that the private API was actually using <strong>Anthropic's Claude</strong> model. There were instances where the model identified itself as <strong>Claude</strong>, an AI assistant created by Anthropic. Some users noticed that words like "Claude" and "Anthropic" were being censored in the responses.</p>

<p>For example, when someone asked the model to say the word "Claude," it was censored. But with careful prompting, the model tried to convey its identity in covert ways. It hinted that it was created by a company whose name starts with "Anthr" and ends with "opic" and that its name rhymes with "odd" and starts with the third letter of the alphabet.</p>

<h3>Community Analysis and Findings</h3>

<p>People on Reddit shared their findings. They pointed out that the model seemed to be a poorly made <strong>Claude wrapper</strong> with a fake open-source model. The open weights model on Hugging Face performed poorly, while the secret API model performed amazingly on benchmarks. This discrepancy raised suspicions.</p>

<p>Users also noticed that when comparing the model's weights, there were minimal differences from the base models. They concluded that the model did not perform as claimed and that possible fraud had occurred.</p>

<h2>Responses from Matt Schumer and Sahil</h2>

<h3>Matt Schumer's Apology</h3>

<p>Matt eventually apologized. He said:</p>

<p>"I got ahead of myself when I announced this project, and I am sorry. That was not my intention. I made a decision to ship this new approach based on the information we had at the moment.</p>

<p>I know that many of you are excited about the potential for this and are now skeptical. Nobody's more excited about the potential for this new approach than I am.</p>

<p>For the moment, we have a team working tirelessly to understand what happened and will determine how to proceed once we get to the bottom of it.</p>

<p>Once we have all the facts, we will continue to be transparent with this community about what happened and the next steps."</p>

<h3>Sahil's Statement</h3>

<p>Sahil, from Glaive AI, also posted a statement. He wanted to address the confusion and valid criticisms. He said:</p>

<p>"I want to address the confusion and valid criticisms that this has caused in the community. We're still currently investigating, etc.</p>

<p>First, I want to be clear that at no point was I running any models from any other providers, as the API that was being served was on my compute. I'm working on providing evidence of this and understanding why people saw model behavior such as using a different tokenizer, completely skipping words like 'Claude.'</p>

<p>Second, the benchmark scores I shared with Matt haven't been reproducible so far. I'm working to understand why this is and if the original scores I reported were accurate or a result of contamination or misconfiguration.</p>

<p>I will do a full postmortem and will share more soon."</p>

<h2>Community Reactions and Conclusions</h2>

<p>The community remained skeptical about their explanations. Many people expressed disappointment and questioned their credibility. Some compared this incident to previous unverified claims, like the <strong>LK-99</strong> superconductor fiasco.</p>

<p>People pointed out that someone was lying—whether it was Matt, Sahil, or both. There were discussions about whether Matt spent all this time building up his reputation to pull off a scam, and if so, what the scam was.</p>

<p>The Reddit community <strong>Local Llama</strong> felt deceived. They had hoped this was real, only to realize they'd been misled. Trust was eroded, and many found it hard to believe any of the people involved.</p>

<h2>Final Reflections</h2>

<p>This incident highlights the importance of <strong>verification and reproducibility</strong> in AI claims. It's a reminder that sensational news can spread quickly, but we need to approach such claims with a healthy dose of skepticism.</p>

<p>I noticed that some influencers and people in the AI community gained views and followers from sharing these sensational claims. They profited from the hype, with zero accountability for sharing sensational information.</p>

<p>Personally, I chose to sit this one out. I hope you noticed that I didn't participate in the hype. It's essential to maintain accountability and trust within the AI community.</p>

<p>With that said, my name is Wesrth, and thank you for reading.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>