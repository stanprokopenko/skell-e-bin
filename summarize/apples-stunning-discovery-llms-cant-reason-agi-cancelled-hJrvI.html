<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">Apple's STUNNING Discovery "LLMs CAN'T REASON" | AGI cancelled</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<p>Apple recently published a paper titled <strong>"The Illusion of Thinking,"</strong> where they tested the reasoning abilities of large language models (LLMs). They wanted to see how well these models perform on tasks requiring reasoning. Many are now questioning if LLMs truly reason or if it's just an illusion.</p>

<h2>Apple's Main Findings</h2>

<p>Apple tested both standard LLMs and those designed for reasoning, often called <strong>large reasoning models</strong>. They focused on tasks like the <strong>Tower of Hanoi</strong>, checkers jumping, river crossing, and blocks world puzzles. Their main findings were:</p>

<ol>

<li><strong>Low Complexity Tasks</strong>: Standard models surprisingly outperformed reasoning models.</li>

<li><strong>Medium Complexity Tasks</strong>: Reasoning models showed advantages due to their ability to think through steps.</li>

<li><strong>High Complexity Tasks</strong>: Both types of models failed, experiencing what they called a "complete collapse."</li>

</ol>

<p>They concluded that these models fail to develop generalized reasoning capabilities beyond a certain complexity threshold.</p>

<h2>Critiques of the Paper</h2>

<p>I have some concerns about their conclusions. First, they criticize math and coding benchmarks, claiming models might have memorized answers from their training data. Yet, they chose well-known puzzles like the Tower of Hanoi for testing. This puzzle is extensively documented online, so solutions are likely part of the models' training data.</p>

<p><strong>Sean Gúndi</strong>, in his blog post, pointed out that using such common puzzles isn't a better alternative. He noted that after a certain complexity, models realize generating thousands of steps isn't practical. Instead, they attempt to find shortcuts or explain the solution approach, much like humans would.</p>

<p>When I asked an AI model to solve the 10-disk Tower of Hanoi problem, it recognized that listing all moves was impractical. Instead, it generated code to solve the puzzle efficiently. This suggests the model is capable of reasoning and finding alternative solutions.</p>

<h2>Human-like Reasoning</h2>

<p>Think about how we approach problems:</p>

<ul>

<li><strong>Simple Problems</strong>: For something like <strong>2 + 2</strong>, we answer instantly.</li>

<li><strong>Moderate Problems</strong>: For <strong>9 × 6</strong>, we might take a moment to think unless we've memorized it.</li>

<li><strong>Complex Problems</strong>: If asked, "How many prime numbers are there between 1 and 15 million?", we'd likely recognize the complexity and not attempt to solve it on the spot.</li>

</ul>

<p>Similarly, AI models assess the complexity of tasks. When a problem is too complex, they might choose not to engage directly or look for alternative methods. This mirrors human reasoning patterns.</p>

<p style="text-align: center;">* * *</p>

<p>The claim that AI reasoning is an illusion seems oversimplified. While models have limitations, especially with highly complex tasks, they do exhibit reasoning abilities similar to ours. They can assess problems, recognize when direct solutions are impractical, and seek alternative approaches.</p>

<p><strong>What do you think?</strong> Is AI reasoning merely an illusion, or are we witnessing machines developing human-like thinking patterns? I'd love to hear your thoughts.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>