<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">Adobe Goes AI Dirty with Flux, Runway, & Google!</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<h2>Adobe Gets Adventurous with External AI Models</h2>

<p>In a surprising twist, <strong>Adobe is now allowing you to create natively in their products using external image generators</strong>. This is a big deal because, until now, Adobe's AI efforts have focused on <strong>Firefly</strong>, emphasizing its clean and ethically safe approach. While Firefly has been improving, its outputs often feel like stock images.</p>

<p>At a recent Gen AI leadership summit at Adobe HQ, I and other creators expressed the need for more options. We wanted the ability to "get dirty" with our creations. Adobe listened and has partnered with <strong>Black Forest Labs</strong> (creators of the <strong>Flux</strong> model), <strong>Google</strong> with <strong>Imagen 3</strong>, and <strong>Runway ML</strong> with <strong>Runway Frames</strong>.</p>

<h3>Integration into Adobe Express and Project Concept</h3>

<p>These models won't appear in Photoshop or Premiere immediately. They will first debut in <strong>Adobe Express</strong> and in <strong>Project Concept</strong>, a beta program I’ve been exploring. Project Concept acts as a mood board mixer where you can bring in images or generate new ones and blend them together to create entirely new styles. With the addition of Flux and Imagen 3, Project Concept just got much more interesting.</p>

<p>Adobe makes it clear that if you're generating outside of the Firefly model, <strong>you take full responsibility</strong>. But the option to use external models is essentially the toggle switch many of us have been asking for.</p>

<h3>The Inclusion of V2</h3>

<p>Interestingly, Adobe also mentioned <strong>V2</strong>, Google's video generator. It's a bit out of place compared to the image generators but hints at future integrations. This echoes Adobe's previous announcement about integrating Sora into Adobe Premiere, which didn't materialize. It shows that <strong>integrations with other generative models have been on Adobe's radar for a while</strong>.</p>

<p>There’s no word on release dates or pricing yet, but depending on how they approach it, it might end up being a cost-saving measure compared to juggling multiple subscriptions. I'll be keeping an eye on this development.</p>

<h2>Stability AI Releases Stable Virtual Camera</h2>

<p><strong>Stability AI</strong> has released a camera control model called <strong>Stable Multi Camera</strong>, which you can use right now for free. It's a multi-view diffusion model that transforms 2D images into immersive 3D videos with realistic depth and perspective, all without complex reconstruction or scene-specific optimization. This is the first glimpse of having <strong>camera control within an image after it has been generated or captured</strong>.</p>

<h3>Running Stable Virtual Camera</h3>

<p>You can run Stable Virtual Camera on <strong>Hugging Face</strong> for free. Simply upload an image, choose your camera move (like orbit, zoom in, or move left), and generate frames to create a video. It can take a bit of time to process, but the results are intriguing.</p>

<p>I tested it with an isometric cityscape image, and while the orbit movement was a bit choppy, stepping through the frames showed some promising results. Even though it's a research preview with limitations, especially with humans and dynamic textures, it opens up exciting possibilities.</p>

<h3>Experimenting with Camera Control</h3>

<p>This got me thinking about previous experiments where I combined Midjourney-generated images with <strong>Gemini 2.0</strong> and <strong>Runway Gen 3</strong> to create more coherent sequences. By adding Stable Virtual Camera into the mix, I could change the camera position and have both <strong>pose control</strong> and <strong>camera control</strong>.</p>

<p>While it's not perfect, there were issues like limbs disconnecting. It illustrates how much control we'll soon have over both camera and subject using keyframes. It's exciting to see where this technology is headed.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>