<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">OpenAI Raising $6.5B at $150B Valuation with $250M Minimum</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<p>For a while now, we've been following the story of OpenAI's latest investment round. Initially, it wasn't exactly clear whether this was just an employee tender offer, or a more significant raise. But that's become clear now, it is definitely a more significant raise. The dynamics of it show that whatever concerns people may have had around AI hype getting too far ahead in the summer, and OpenAI starting to surrender its lead to competitors, the latest O1 model and its other activities seem to have answered those entirely.</p>

<h2>OpenAI's Latest Funding Round</h2>

<p>First of all, the latest funding round seems set to value OpenAI at around <strong>$150 billion</strong>. That's obviously a significant increase from the <strong>$80 to $90 billion</strong> of the last round. The size of the total round has also increased. We had previously heard that Thrive Capital was leading the round, but now it appears that the total financing will be somewhere between <strong>$5 and $7 billion</strong>, led by Thrive, which is committing <strong>$1 billion</strong>.</p>

<h3>Major Investors and Minimum Investment</h3>

<p>Microsoft, Nvidia, and Apple are all looking to invest, with a total aggregate contribution between <strong>$2 and $3 billion</strong>. One big question is what's the combination of cash and other resources like computing power that's going into that. A couple of other likely investors named are Tiger Global Management, as well as <strong>G42</strong>, which is an Abu Dhabi-based AI firm.</p>

<p>In the case of Tiger, it appears that they're doing this through a special purpose vehicle, which will aggregate other investors into a single investment. As <em>The Information</em> puts it, this could help investors make good on their commitments without overweighting their main funds in OpenAI shares.</p>

<p>One of the new details is an apparent <strong>$250 million minimum investment</strong>. For some context on that, according to PitchBook, <strong>$250 million is six times larger than the median venture fund raised in the US last year</strong>. VC Matt Turk framed it this way in a tweet:</p>

<blockquote>

<ul>

<li>1990s: Entire assets under management of a firm across funds.</li>

<li>2010s: Total size of a Series A fund (30+ companies).</li>

<li>2020â€“2021: Total size of one round in one late-stage company.</li>

<li>2024: Size of each participant's investment in one round of one company.</li>

</ul>

</blockquote>

<h3>High Demand and Notable Absences</h3>

<p>What we get from Bloomberg is that even if you do have a quarter billion dollars to spend on this, you're not guaranteed to get into the round. Bloomberg writes that investors were set to find out today whether they'd actually get into the deal. One of their sources said that excess demand was in the <strong>billions of dollars</strong>. So if the number does come in at around <strong>$6.5 billion</strong>, that means there were potentially billions of dollars more that were left on the table.</p>

<p>Bloomberg also noted that there was one notable participant who won't be investing, which is <strong>Sequoia</strong>. Bloomberg frames it potentially as a choice to focus on rival company <strong>Safe Superintelligence</strong>, which was, of course, founded by OpenAI co-founder <strong>Ilya Sutskever</strong>, who left the company earlier this year. Maybe also this is Sequoia making good on their concerns that they expressed in the <strong>$600 Billion Challenge</strong> blog post from over the summer.</p>

<p>The Financial Times put the logic of all this into very simple terms. Venture groups, they write, believe OpenAI will eventually be the world's dominant AI company, and worth <strong>trillions of dollars</strong>. Ultimately, this is really all handicapping the odds that OpenAI wins, which really means OpenAI continues to lead in a market that they believe is going to be worth trillions of dollars. In that light, <strong>$150 billion</strong> starts to look, if not reasonable, at least a lot less insane.</p>

<p>The Financial Times added that it appears that <strong>Andreessen Horowitz</strong> is also sitting out of the round. One of the investors that the Financial Times spoke with said that generative AI represented the <strong>biggest platform prize since cloud or the internet</strong>, worth multiple trillions of dollars of economic value.</p>

<h3>Apple Joining as an Investor</h3>

<p>Another interesting note from anonymous investors in this conversation has to do with the introduction of Apple as an investor. The Financial Times writes:</p>

<p>"More important still could be closer ties to strategic investors," said one investor in OpenAI. OpenAI have Microsoft, the biggest enterprise company on the planet. If I could pick another partner, it would be Apple, the biggest consumer company on the planet. I'm walking into a gunfight with Google and Facebook, and I have my Microsoft and Apple behind me, it's not such a bad thing from a distribution and branding perspective.</p>

<h2>Sam Altman Discusses AI Development Levels</h2>

<p>Now, I'm sure we will get more information soon as that round finalizes. But in the meantime, there was also an interesting conversation between <strong>Sam Altman</strong> and leaders at T-Mobile at an event yesterday, where Altman used the recently introduced framework of levels to explain that O1 does represent their <strong>Level Two</strong>, and that while the shift to Level Two took time, he thinks that that means that <strong>Level Three</strong> will be coming faster.</p>

<p>"We have these five levels of AI we talk about." The first was chatbots. The second, which we've just reached now, is reasoners. The third is agents. The fourth is sort of innovators, the ability to figure out new scientific information. And the fifth is full organizations. So this move from one to two took a while, but I think one of the most exciting things about two is that it enables Level Three relatively quickly after. And the agentic experiences that we expect this technology to eventually enable, I think will be quite impactful.</p>

<h2>Advancements in OpenAI's Reasoning Models</h2>

<p><em>The Information</em> recently explored this in somewhat more detail, writing:</p>

<blockquote>

<p>"One of the most important applications of Strawberry, which is of course now called O1, is to generate high-quality training data for <strong>Orion</strong>, OpenAI's next flagship large language model that's in development. Using Strawberry or O1 could help Orion reduce the number of hallucinations or errors it produces, researchers tell me. That's because AI models learn from their training data, so the more correct examples of complex reasoning they see, the better."</p>

</blockquote>

<h3>Easter Eggs Hinting at Orion</h3>

<p>They also pointed out an Altman Easter egg around Orion from September 13th:</p>

<p>"I love being home in the Midwest." The night sky is so beautiful. Excited for the winter constellations to rise soon, they are so great.</p>

<p>When you ask ChatGPT what constellations he might be referring to, it says he is likely referring to prominent constellations that are visible during the winter months in the Northern Hemisphere. The first one it listed is, of course, <strong>Orion</strong>.</p>

<p>Since we're on the topic, a couple of additional pieces of OpenAI news to round it out: The miniature version of OpenAI's new reasoning model is, as the week has gone on, getting more and more people excited. Amir from <em>The Information</em> again writes:</p>

<blockquote>

<p>"OpenAI's week of good news keeps getting better. After releasing an initial version of its O1 reasoning model, O1 Preview, new details are emerging about a smaller but faster version of the model, <strong>O1 Mini</strong>. Turns out that O1 Mini is even better than O1 Preview in math, according to developers, and is generally on par in most other ways."</p>

</blockquote>

<p>One reason they write for O1 Mini's relatively mighty performance is that OpenAI allows customers to use more tokens compared to O1 Preview. O1 Mini's smaller size means it processes information more efficiently and more cheaply. Because of that, OpenAI can let O1 Mini <strong>think longer than O1 Preview</strong>, and that can help prove what the company says is the best part of its reasoning models: <strong>more thinking time equals better answers</strong>, otherwise known as <strong>log-linear compute scaling</strong>.</p>

<p>The efficiency gains OpenAI got from developing a miniature form of a reasoning model appeared to be just as big of an accomplishment as proving out that reasoning concept.</p>

<h2>Leah Belsky Joins OpenAI as General Manager of Education</h2>

<p>Lastly today, congrats to friend of the show <strong>Leah Belsky</strong>, a former Coursera executive who I met years ago in my first stint with Learn Capital, who has just been recruited and hired by OpenAI to be their first <strong>General Manager of Education</strong> and lead their efforts to bring ChatGPT into schools and classrooms. I, for one, am definitely on the optimistic side about how AI can help education, and so I love that there is going to be more focus there.</p>

<p>For now, though, that is going to do it for today's AI Daily Brief. Until next time, peace.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>