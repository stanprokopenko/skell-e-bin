<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">Llama 4 Released - it is not what I expected at all</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<h2>Llama 4 Release and Models</h2>

<p>Today, Llama 4 was released, and I was eager to test its coding abilities. There are three models:</p>

<ul>

<li><strong>Llama 4 Scout</strong>: the smallest model.</li>

<li><strong>Llama 4 Maverick</strong>: the medium-sized model.</li>

<li><strong>Llama 4 Behemoth</strong>: still training and not yet available.</li>

</ul>

<h2>Testing Maverick's Coding Abilities</h2>

<p>As an AI coding enthusiast, I care most about <strong>coding benchmarks</strong>. In the benchmarks, Scout scored <strong>32.88</strong>, slightly better than <strong>28.9</strong> from <strong>Gemini 2.0 Flashlight</strong>. Maverick scored <strong>34.5</strong> compared to <strong>43.4</strong>. I had high expectations for Maverick's coding performance.</p>

<p>I tested Maverick using JanAI, asking it to implement a <strong>self-contained Python program using Pygame</strong>. Unfortunately, the code it generated was underwhelming. For example, a pool game it created lacked proper physics and didn't function well.</p>

<p>In contrast, when I tried the same prompt with <strong>Gemini Flash 2.0</strong>, the code was much better. The pool game was functional, with better physics and interaction.</p>

<h2>Technical Issues with Root Code and Context Windows</h2>

<p>I tried using Root Code with Maverick via Open Router but faced constant failures. Every provider (DeepInfra, Parasol, Together, Fireworks) failed to execute code without errors, making it impossible to test Maverick's capabilities effectively.</p>

<p>Llama 4 Scout boasts a <strong>10 million token context window</strong>, which is mind-blowing. However, in practice, I couldn't test this effectively. When adding context, the model seemed to reset after about <strong>500k tokens</strong>, dropping earlier context and restarting. This limitation hindered my ability to evaluate the model's extended context capabilities.</p>

<h2>Licensing Considerations</h2>

<p>The models are <strong>open-source</strong>, though with some licensing caveats from Meta:</p>

<ul>

<li>Companies with over <strong>700 million monthly active users</strong> must request a special license.</li>

<li>Any AI model using Llama must include <strong>"Llama"</strong> at the beginning of its name.</li>

<li>Usage must comply with Meta's <strong>acceptable use policy</strong>.</li>

</ul>

<p style="text-align: center;">* * *</p>

<p>Overall, I'm disappointed with Llama 4's coding abilities. <strong>Maverick</strong> performed worse than <strong>Gemini 2.0 Flash</strong>, despite benchmarks suggesting otherwise. I hope there's an issue with the models available on Open Router, or perhaps the model isn't optimized for coding tasks.</p>

<p>On the positive side:</p>

<ul>

<li>The models are <strong>inexpensive</strong>, making them accessible.</li>

<li>They offer <strong>large context windows</strong>, which is promising.</li>

</ul>

<p>I'm bummed that Llama 4 didn't meet my expectations for coding tasks. I was excited about a new model with potential, but it seems we might have to wait for Behemoth or future updates. For now, I'll stick with models like <strong>Gemini 2.0 Flash</strong> for coding.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>